{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possibly the last iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The plan:\n",
    "\n",
    "- Describe the dataset\n",
    "- Talk first easy NN just to see the difference in the data-cleaning part\n",
    "- Data cleaning, each significant step will have its own nn try\n",
    "- Different architectures\n",
    "- Optimizing hyperparameters\n",
    "\n",
    "*The LSTM ans the creation of the windows (timeseries) will be done in a separate window*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor Data\n",
    "\n",
    "The sensors are monitoring environmental factors, we can specify the following types of sensor data:\n",
    "1. Temperature (TEMP)\n",
    "2. Humidity (HUM)\n",
    "...\n",
    "\n",
    "The costly sensor is assumed to measure **Black Carbon concentration (BC)**, which requires more advanced technology and calibration, thus making it more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(\"BC-Data-Set.csv\")\n",
    "data = data.set_index(pd.to_datetime(data['date']))\n",
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#now we can fill the days that are missing some hours, we will do that using the mean\n",
    "test = data.groupby([data.index.date])\n",
    "for group_name, group_data in test:\n",
    "    if len(group_data)!=24:\n",
    "        existing_hours=group_data.index.hour.unique()\n",
    "        missing_hours = set(range(24)) - set(existing_hours)\n",
    "        missing_rows = pd.DataFrame(columns=group_data.columns)\n",
    "        for missing_hour in missing_hours:\n",
    "            datetime_obj = pd.to_datetime(group_data.index.date[0]) + pd.to_timedelta(missing_hour, unit='H')\n",
    "            data.loc[datetime_obj] = data.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that outliers are present, let's remove them.\n",
    "We will scale the values too, the NN needs this to work efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_index()\n",
    "data = data.reset_index(drop=True)\n",
    "dates = data[\"date\"]\n",
    "data = data.drop([\"date\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_dataframe(df, columns):\n",
    "    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17,17))\n",
    "    for i, col in enumerate(columns):\n",
    "        axs[i].plot(df[col])\n",
    "        axs[i].set_title(col)\n",
    "    plt.show()\n",
    "inspect_dataframe(data, data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 6 # theshold a little high to retain some outliers\n",
    "z_scores = np.abs((data - data.mean()) / data.std())\n",
    "outliers = (z_scores > threshold).any(axis=1)\n",
    "for column in data.columns:\n",
    "    column_median = data[column].median()\n",
    "    data.loc[outliers, column] = column_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data, this is a rudimentary MinMaxScaler\n",
    "max_df = data.max()\n",
    "min_df = data.min()\n",
    "\n",
    "data_norm = (data - min_df)/(max_df - min_df)\n",
    "data_norm = pd.DataFrame(data_norm, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = data_norm.corr()\n",
    "# Scatter plots\n",
    "sns.pairplot(data_norm, x_vars=data_norm.columns[1:], y_vars=['BC'])\n",
    "\n",
    "# Heat map\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(correlations, annot=True, cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_dataframe(data_norm, data.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Talk first easy NN just to see the difference in the data-cleaning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tf.config.set_visible_devices([], 'GPU') #disables GPU\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to prove that is the same as the classic MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "target = pd.DataFrame(data.BC)\n",
    "X = data.drop(['BC'], axis=1)\n",
    "print('Boston data_normset shape',X.shape)\n",
    "print('Target shape', target.shape)\n",
    "X.describe()\n",
    "\n",
    "scaler_x = MinMaxScaler()\n",
    "x_scaled = scaler_x.fit_transform(X)\n",
    "x_scaled = pd.DataFrame(x_scaled, columns=X.columns)\n",
    "\n",
    "#do the same for y\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "y_scaled = scaler_y.fit_transform(target) \n",
    "y_scaled = pd.DataFrame(y_scaled, columns=target.columns)\n",
    "\n",
    "\n",
    "y_scaled.describe()   \n",
    "x_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split in test and train\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x_scaled, y_scaled, test_size = 0.2, random_state=seed, shuffle=True)\n",
    "# print(X_train.shape, y_train.shape)\n",
    "# print(X_test.shape, y_test.shape)\n",
    "\n",
    "test_size = 24*19\n",
    "\n",
    "X_train = x_scaled.iloc[:-test_size]\n",
    "y_train = y_scaled.iloc[:-test_size]\n",
    "X_test = x_scaled.iloc[-test_size:]\n",
    "y_test = y_scaled.iloc[-test_size:]\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the target\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.histplot(data=y_train, x='BC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ffnn(input_shape):\n",
    "\n",
    "    # Build the neural network layer by layer\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
    "    hidden_layer1 = tfkl.Dense(units=128, activation='relu', name='Hidden1')(input_layer)\n",
    "    hidden_layer2 = tfkl.Dense(units=64, activation='relu', name='Hidden2')(hidden_layer1)\n",
    "    output_layer = tfkl.Dense(units=1, activation='linear', name='Output')(hidden_layer2)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='FFNN')\n",
    "\n",
    "    # Compile the model\n",
    "    loss = tfk.losses.MeanSquaredError()\n",
    "    learning_rate = 0.2\n",
    "    optimizer = tfk.optimizers.legacy.SGD(learning_rate)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[tfk.metrics.MeanAbsoluteError(), tfk.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnn = build_ffnn(input_shape)\n",
    "ffnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ffnn.fit(\n",
    "    x = X_train,\n",
    "    y = y_train, \n",
    "    batch_size = batch_size,\n",
    "    validation_split=0.3,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=20,  restore_best_weights=True),\n",
    "        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-5)\n",
    "                            ],\n",
    "    epochs = epochs\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ffnn.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=ffnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print(\"MSE\",mean_squared_error(y_test,y_pred))\n",
    "print(\"R2\",r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"MSE\",mean_squared_error(scaler_y.inverse_transform(y_test),scaler_y.inverse_transform(y_pred)))\n",
    "print(\"R2\",r2_score(scaler_y.inverse_transform(y_test),scaler_y.inverse_transform(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(model, X_, y_):\n",
    "    X_['sort'] = y_\n",
    "    X_ = X_.sort_values(by=['sort'])\n",
    "    y_ = np.expand_dims(X_['sort'], 1)\n",
    "    X_.drop(['sort'], axis=1, inplace=True)\n",
    "\n",
    "    y_pred = model.predict(X_)\n",
    "    SSE = np.square(scaler_y.inverse_transform(y_pred) - scaler_y.inverse_transform(y_))\n",
    "    MSE = np.mean(SSE)\n",
    "\n",
    "    print('Mean Squared Error (MSE):', MSE)\n",
    "    # mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "    # sns.set(font_scale=1.1, style=None, palette='Set1')\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.scatter(np.arange(len(y_)), y_, label='True')\n",
    "    plt.scatter(np.arange(len(y_pred)), y_pred, label='Prediction')\n",
    "    \n",
    "    for i in range(len(y_)):\n",
    "        if(y_[i]>=y_pred[i]):\n",
    "            plt.vlines(i,y_pred[i],y_[i],alpha=.5)\n",
    "        else:\n",
    "            plt.vlines(i,y_[i],y_pred[i],alpha=.5)\n",
    "            \n",
    "    plt.legend()\n",
    "    plt.grid(alpha=.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(ffnn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(ffnn, X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
