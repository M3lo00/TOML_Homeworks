{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")  # Report only TF errors by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(\"BC-Data-Set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to a datetime object\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Remove any missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Set the date column as the index of the DataFrame\n",
    "data = data.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, data.BC, test_size=0.15, random_state=seed, shuffle=True)\n",
    "x_train = x_train.drop(columns=['BC'])\n",
    "x_test = x_test.drop(columns=['BC'])\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15, random_state=seed, shuffle=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: split, preprocess, make sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.73291434,  1.55477006,  3.60894021, ...,  1.95560752,\n",
       "        -0.89521109,  0.69813568],\n",
       "       [-0.85503739, -0.08410553, -0.35847915, ..., -0.37050872,\n",
       "        -0.48279646,  0.2517087 ],\n",
       "       [-0.38903519, -0.40824082, -0.54120237, ...,  0.15400768,\n",
       "        -1.71157723, -1.90387708],\n",
       "       ...,\n",
       "       [-0.89772   , -0.72621189, -0.91761219, ..., -0.39331378,\n",
       "         0.94389125, -0.81840302],\n",
       "       [-0.48238073,  0.10302261, -0.19784883, ...,  1.11182025,\n",
       "         1.35890992, -0.49238002],\n",
       "       [-0.7281987 , -0.953701  , -1.27475302, ..., -0.48453403,\n",
       "        -1.55972922, -2.74749276]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3050, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network (non-LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 8\n",
    "inputs = tf.keras.layers.Input(shape=[x_train.shape[-1]])\n",
    "\n",
    "layer = keras.layers.Dense(HIDDEN_DIM, activation='relu')(inputs)\n",
    "layer = keras.layers.Dense(HIDDEN_DIM, activation='relu')(layer)\n",
    "\n",
    "predictions = keras.layers.Dense(1, activation=None)(layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(\n",
    "            optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss=tf.losses.mean_squared_error,\n",
    "            metrics=[tf.metrics.mean_squared_error],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "48/96 [==============>...............] - ETA: 0s - loss: 1.9227 - mean_squared_error: 1.9227 "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "logs = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size, epochs=epochs,\n",
    "    validation_data=(x_val, y_val),\n",
    "    # callbacks=[model.tb_callback],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def build_sequences(df, window=200, stride=200):\n",
    "    # Sanity check to avoid runtime errors\n",
    "    assert window % stride == 0\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    for id in df['id'].unique():\n",
    "        # Take only meaningful features\n",
    "        temp = df[df['id'] == id][['x_axis','y_axis','z_axis']].values\n",
    "        # Save the label\n",
    "        label = df[df['id'] == id]['activity'].values[0]\n",
    "        print(str(id), \"  \", str(label))\n",
    "        # Compute padding length\n",
    "        padding_len = window - len(temp)%window\n",
    "        # Create padding and concatenate it\n",
    "        padding = np.zeros((padding_len,3), dtype='float64')\n",
    "        temp = np.concatenate((temp,padding))\n",
    "        # Build features windows with their corresponging labels\n",
    "        idx = 0\n",
    "        while idx+window <= len(temp):\n",
    "            dataset.append(temp[idx:idx+window])\n",
    "            labels.append(label)\n",
    "            idx += stride\n",
    "    dataset = np.array(dataset)\n",
    "    labels = np.array(labels)\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 8\n",
    "SEQUENCE_DIM = x_train.shape[-1]\n",
    "RNN_CELL_DIM = 8\n",
    "HIDDEN_DIM = 8\n",
    "sequences = tf.keras.layers.Input(shape=[SEQUENCE_LENGTH, SEQUENCE_DIM])\n",
    "\n",
    "layer = keras.layers.LSTM(RNN_CELL_DIM, return_sequences=True)(sequences)\n",
    "\n",
    "layer = keras.layers.Dense(HIDDEN_DIM, activation='relu')(layer)\n",
    "\n",
    "predictions = keras.layers.Dense(1, activation=None)(layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=sequences, outputs=predictions)\n",
    "model.compile(\n",
    "            optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss=tf.losses.mean_squared_error,\n",
    "            metrics=[tf.metrics.mean_squared_error],\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
