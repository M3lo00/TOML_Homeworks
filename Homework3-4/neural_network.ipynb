{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")  # Report only TF errors by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tf.config.set_visible_devices([], 'GPU') #disables GPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(\"filled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to a datetime object\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Remove any missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Set the date column as the index of the DataFrame\n",
    "data = data.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, data.BC, test_size=0.15, random_state=seed, shuffle=True)\n",
    "x_train = x_train.drop(columns=['BC'])\n",
    "x_test = x_test.drop(columns=['BC'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: split, preprocess, make sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum() # check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 24*25\n",
    "X_train_raw = data.iloc[:-test_size]\n",
    "# y_train_raw = y.iloc[:-test_size]\n",
    "X_test_raw = data.iloc[-test_size:]\n",
    "# y_test_raw = y.iloc[-test_size:]\n",
    "print(X_train_raw.shape, X_test_raw.shape)\n",
    "\n",
    "# Normalize both features and labels\n",
    "X_min = X_train_raw.min()\n",
    "X_max = X_train_raw.max()\n",
    "\n",
    "X_train_raw = (X_train_raw-X_min)/(X_max-X_min)\n",
    "X_test_raw = (X_test_raw-X_min)/(X_max-X_min)\n",
    "\n",
    "plt.figure(figsize=(17,5))\n",
    "plt.plot(X_train_raw.BC, label='Train (BC)')\n",
    "plt.plot(X_test_raw.BC, label='Test (BC)')\n",
    "plt.title('Train-Test Split')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network (non-LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, shuffle= True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 64\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=[x_train.shape[-1]])\n",
    "\n",
    "hidden_layer1 = tfkl.Dense(units=256, activation='relu', kernel_initializer=tfk.initializers.HeUniform(seed=seed), name='Hidden1')(inputs)\n",
    "hidden_layer2 = tfkl.Dense(units=128, activation='relu', kernel_initializer=tfk.initializers.HeUniform(seed=seed), name='Hidden2')(hidden_layer1)\n",
    "hidden_layer3 = tfkl.Dense(units=64, activation='relu', kernel_initializer=tfk.initializers.HeUniform(seed=seed), name='Hidden3')(hidden_layer2)\n",
    "output_layer = tfkl.Dense(units=1, activation='linear', kernel_initializer=tfk.initializers.GlorotUniform(seed=seed),name='Output')(hidden_layer3)\n",
    "predictions = keras.layers.Dense(1, activation='linear')(output_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(\n",
    "            optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-3),\n",
    "            loss=tf.losses.mean_squared_error,\n",
    "            metrics=[tf.metrics.mean_squared_error],\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 2000\n",
    "logs = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size, epochs=epochs,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=50,  restore_best_weights=True)\n",
    "                            ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logs.history['loss'])\n",
    "plt.plot(logs.history['val_loss'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 24*30 #the number of days is 191\n",
    "\n",
    "train = data.iloc[:-test_size]\n",
    "# y_train_raw = y.iloc[:-test_size]\n",
    "test = data.iloc[-test_size:]\n",
    "# y_test_raw = y.iloc[-test_size:]\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "def build_sequences(df, target_labels=['BC'], window=200, stride=200):\n",
    "    # Sanity check to avoid runtime errors\n",
    "    assert window % stride == 0\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    temp_df = df.drop(target_labels, axis=1).copy().values\n",
    "    temp_label = df[target_labels].copy().values\n",
    "    padding_len = len(df) % window\n",
    "    \n",
    "    if padding_len != 0:\n",
    "        # Compute padding length\n",
    "        padding_len = window - len(df) % window\n",
    "        padding = np.zeros((padding_len, temp_df.shape[1]), dtype='float32')\n",
    "        temp_df = np.concatenate((padding, temp_df))\n",
    "        padding = np.zeros((padding_len,1), dtype='float32')\n",
    "        #padding = np.zeros((padding_len, temp_label.shape[1]), dtype='float32')\n",
    "        temp_label = np.concatenate((padding, temp_label))\n",
    "        assert len(temp_df) % window == 0\n",
    "\n",
    "    # Build sequences and labels\n",
    "    for i in range(0, len(temp_df) - window + 1, stride):\n",
    "        dataset.append(temp_df[i:i + window])\n",
    "        labels.append(temp_label[i:i + window])\n",
    "\n",
    "    return np.array(dataset), np.array(labels)\n",
    "\n",
    "X_train, y_train = build_sequences(train, window=24, stride=24)\n",
    "X_test, y_test = build_sequences(test, window=24, stride=24)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "window = 12\n",
    "stride = 12\n",
    "target_labels = 'BC'\n",
    "\n",
    "def build_sequences(df, target_labels=['BC'], window=200, stride=200):\n",
    "    # Sanity check to avoid runtime errors\n",
    "    assert window % stride == 0\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    temp_df = df.copy().values\n",
    "    temp_label = df[target_labels].copy().values\n",
    "    padding_len = len(df) % window\n",
    "    \n",
    "    if padding_len != 0:\n",
    "        # Compute padding length\n",
    "        padding_len = window - len(df) % window\n",
    "        padding = np.zeros((padding_len, temp_df.shape[1]), dtype='float32')\n",
    "        temp_df = np.concatenate((padding, temp_df))\n",
    "        padding = np.zeros((padding_len,), dtype='float32')\n",
    "        #padding = np.zeros((padding_len, temp_label.shape[1]), dtype='float32')\n",
    "        temp_label = np.concatenate((padding, temp_label))\n",
    "        assert len(temp_df) % window == 0\n",
    "\n",
    "    # Build sequences and labels\n",
    "    for i in range(0, len(temp_df) - window + 1, stride):\n",
    "        dataset.append(temp_df[i:i + window])\n",
    "        labels.append(temp_label[i:i + window])\n",
    "\n",
    "    return np.array(dataset), np.array(labels)\n",
    "\n",
    "X_train, y_train = build_sequences(data, target_labels, window, stride)\n",
    "X_test, y_test = build_sequences(data, target_labels, window, stride)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "output_shape = y_train.shape[1:]\n",
    "batch_size = 16\n",
    "epochs = 200\n",
    "\n",
    "input_shape, output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as tfkl\n",
    "import tensorflow.keras as tfk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LSTM_classifier(input_shape):\n",
    "    # Build the neural network layer by layer\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
    "\n",
    "    # Feature extractor\n",
    "    lstm = tfkl.LSTM(128, return_sequences=True)(input_layer)\n",
    "    lstm = tfkl.LSTM(128)(lstm)\n",
    "    dropout = tfkl.Dropout(.5, seed=seed)(lstm)\n",
    "\n",
    "    # Classifier\n",
    "    classifier = tfkl.Dense(128, activation='relu')(dropout)\n",
    "    output_layer = tfkl.Dense(1)(classifier)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.MeanSquaredError(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_LSTM_classifier(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_split=.1,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True),\n",
    "        tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', patience=5, factor=0.5, min_lr=1e-5)\n",
    "    ]\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CONV_LSTM_model(input_shape, output_shape):\n",
    "    # Build the neural network layer by layer\n",
    "    input_layer = tf.keras.layers.Input(shape=input_shape, name='input_layer')\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, name='lstm'), name='bidirectional_lstm')(input_layer)\n",
    "    x = tf.keras.layers.Conv1D(128, 3, padding='same', activation='relu', name='conv')(x)\n",
    "    if output_shape[0] == 1:\n",
    "        output_layer = tf.keras.layers.Conv1D(output_shape[1], 3, padding='same', activation='sigmoid', name='output_layer')(x)\n",
    "        output_layer = tf.keras.layers.GlobalAveragePooling1D(keepdims=True, name='gap')(output_layer)\n",
    "    else:\n",
    "        output_layer = tf.keras.layers.Conv1D(output_shape[1], 3, padding='same', activation='sigmoid', name='output_layer')(x)\n",
    "        crop_size = (output_layer.shape[1]-output_shape[0])//2\n",
    "        output_layer = tf.keras.layers.Cropping1D((crop_size,crop_size), name='cropping')(output_layer)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(), metrics=['mae'])\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_CONV_LSTM_model(input_shape, output_shape)\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_split=.1,\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=50, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=10, factor=0.1, min_lr=1e-5)\n",
    "    ]\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x = x_train,\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_split=.1,\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=50, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=10, factor=0.1, min_lr=1e-5)\n",
    "    ]\n",
    ").history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 12\n",
    "SEQUENCE_DIM = x_train.shape[-1]\n",
    "RNN_CELL_DIM = 8\n",
    "HIDDEN_DIM = 8\n",
    "sequences = tf.keras.layers.Input(shape=[SEQUENCE_LENGTH, SEQUENCE_DIM])\n",
    "\n",
    "layer = keras.layers.LSTM(RNN_CELL_DIM, return_sequences=True)(sequences)\n",
    "\n",
    "layer = keras.layers.Dense(HIDDEN_DIM, activation='relu')(layer)\n",
    "\n",
    "predictions = keras.layers.Dense(1, activation=None)(layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=sequences, outputs=predictions)\n",
    "model.compile(\n",
    "            optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-3),\n",
    "            loss=tf.losses.mean_squared_error,\n",
    "            metrics=[tf.metrics.mean_squared_error],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
