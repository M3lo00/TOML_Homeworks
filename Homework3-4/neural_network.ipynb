{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")  # Report only TF errors by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/melo/miniconda3/envs/my_tf_env/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(\"BC-Data-Set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to a datetime object\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Remove any missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Set the date column as the index of the DataFrame\n",
    "data = data.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, data.BC, test_size=0.15, random_state=seed, shuffle=True)\n",
    "x_train = x_train.drop(columns=['BC'])\n",
    "x_test = x_test.drop(columns=['BC'])\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15, random_state=seed, shuffle=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: split, preprocess, make sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.73291434,  1.55477006,  3.60894021, ...,  1.95560752,\n",
       "        -0.89521109,  0.69813568],\n",
       "       [-0.85503739, -0.08410553, -0.35847915, ..., -0.37050872,\n",
       "        -0.48279646,  0.2517087 ],\n",
       "       [-0.38903519, -0.40824082, -0.54120237, ...,  0.15400768,\n",
       "        -1.71157723, -1.90387708],\n",
       "       ...,\n",
       "       [-0.89772   , -0.72621189, -0.91761219, ..., -0.39331378,\n",
       "         0.94389125, -0.81840302],\n",
       "       [-0.48238073,  0.10302261, -0.19784883, ...,  1.11182025,\n",
       "         1.35890992, -0.49238002],\n",
       "       [-0.7281987 , -0.953701  , -1.27475302, ..., -0.48453403,\n",
       "        -1.55972922, -2.74749276]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3050, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network (non-LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 256\n",
    "inputs = tf.keras.layers.Input(shape=[x_train.shape[-1]])\n",
    "\n",
    "layer = keras.layers.Dense(HIDDEN_DIM, activation='relu')(inputs)\n",
    "layer = keras.layers.Dense(HIDDEN_DIM, activation='relu')(layer)\n",
    "layer = keras.layers.Dense(HIDDEN_DIM, activation='relu')(layer)\n",
    "layer = keras.layers.Dense(HIDDEN_DIM, activation='relu')(layer)\n",
    "\n",
    "layer = keras.layers.Dense(HIDDEN_DIM, activation='relu')(layer)\n",
    "\n",
    "predictions = keras.layers.Dense(1, activation=None)(layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(\n",
    "            optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-3),\n",
    "            loss=tf.losses.mean_squared_error,\n",
    "            metrics=[tf.metrics.mean_squared_error],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "96/96 [==============================] - 2s 14ms/step - loss: 0.6867 - mean_squared_error: 0.6867 - val_loss: 0.4742 - val_mean_squared_error: 0.4742\n",
      "Epoch 2/2000\n",
      "96/96 [==============================] - 1s 9ms/step - loss: 0.4431 - mean_squared_error: 0.4431 - val_loss: 0.5747 - val_mean_squared_error: 0.5747\n",
      "Epoch 3/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.3690 - mean_squared_error: 0.3690 - val_loss: 0.4572 - val_mean_squared_error: 0.4572\n",
      "Epoch 4/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.3182 - mean_squared_error: 0.3182 - val_loss: 0.3761 - val_mean_squared_error: 0.3761\n",
      "Epoch 5/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.2795 - mean_squared_error: 0.2795 - val_loss: 0.3797 - val_mean_squared_error: 0.3797\n",
      "Epoch 6/2000\n",
      "96/96 [==============================] - 1s 9ms/step - loss: 0.2782 - mean_squared_error: 0.2782 - val_loss: 0.3786 - val_mean_squared_error: 0.3786\n",
      "Epoch 7/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.2624 - mean_squared_error: 0.2624 - val_loss: 0.3909 - val_mean_squared_error: 0.3909\n",
      "Epoch 8/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.2639 - mean_squared_error: 0.2639 - val_loss: 0.3226 - val_mean_squared_error: 0.3226\n",
      "Epoch 9/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.2140 - mean_squared_error: 0.2140 - val_loss: 0.3210 - val_mean_squared_error: 0.3210\n",
      "Epoch 10/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.2173 - mean_squared_error: 0.2173 - val_loss: 0.3469 - val_mean_squared_error: 0.3469\n",
      "Epoch 11/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.2064 - mean_squared_error: 0.2064 - val_loss: 0.3060 - val_mean_squared_error: 0.3060\n",
      "Epoch 12/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.2193 - mean_squared_error: 0.2193 - val_loss: 0.3320 - val_mean_squared_error: 0.3320\n",
      "Epoch 13/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1871 - mean_squared_error: 0.1871 - val_loss: 0.3435 - val_mean_squared_error: 0.3435\n",
      "Epoch 14/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1852 - mean_squared_error: 0.1852 - val_loss: 0.2541 - val_mean_squared_error: 0.2541\n",
      "Epoch 15/2000\n",
      "96/96 [==============================] - 1s 9ms/step - loss: 0.1687 - mean_squared_error: 0.1687 - val_loss: 0.3218 - val_mean_squared_error: 0.3218\n",
      "Epoch 16/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1894 - mean_squared_error: 0.1894 - val_loss: 0.3099 - val_mean_squared_error: 0.3099\n",
      "Epoch 17/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1508 - mean_squared_error: 0.1508 - val_loss: 0.2536 - val_mean_squared_error: 0.2536\n",
      "Epoch 18/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1689 - mean_squared_error: 0.1689 - val_loss: 0.2814 - val_mean_squared_error: 0.2814\n",
      "Epoch 19/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1371 - mean_squared_error: 0.1371 - val_loss: 0.3000 - val_mean_squared_error: 0.3000\n",
      "Epoch 20/2000\n",
      "96/96 [==============================] - 1s 9ms/step - loss: 0.1273 - mean_squared_error: 0.1273 - val_loss: 0.2598 - val_mean_squared_error: 0.2598\n",
      "Epoch 21/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1283 - mean_squared_error: 0.1283 - val_loss: 0.3398 - val_mean_squared_error: 0.3398\n",
      "Epoch 22/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1440 - mean_squared_error: 0.1440 - val_loss: 0.2863 - val_mean_squared_error: 0.2863\n",
      "Epoch 23/2000\n",
      "96/96 [==============================] - 1s 9ms/step - loss: 0.1156 - mean_squared_error: 0.1156 - val_loss: 0.2653 - val_mean_squared_error: 0.2653\n",
      "Epoch 24/2000\n",
      "96/96 [==============================] - 1s 10ms/step - loss: 0.1174 - mean_squared_error: 0.1174 - val_loss: 0.3245 - val_mean_squared_error: 0.3245\n",
      "Epoch 25/2000\n",
      "96/96 [==============================] - 1s 9ms/step - loss: 0.1181 - mean_squared_error: 0.1181 - val_loss: 0.2889 - val_mean_squared_error: 0.2889\n",
      "Epoch 26/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.1138 - mean_squared_error: 0.1138 - val_loss: 0.2664 - val_mean_squared_error: 0.2664\n",
      "Epoch 27/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1195 - mean_squared_error: 0.1195 - val_loss: 0.2766 - val_mean_squared_error: 0.2766\n",
      "Epoch 28/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.1049 - mean_squared_error: 0.1049 - val_loss: 0.2781 - val_mean_squared_error: 0.2781\n",
      "Epoch 29/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0877 - mean_squared_error: 0.0877 - val_loss: 0.2643 - val_mean_squared_error: 0.2643\n",
      "Epoch 30/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0972 - mean_squared_error: 0.0972 - val_loss: 0.2911 - val_mean_squared_error: 0.2911\n",
      "Epoch 31/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0862 - mean_squared_error: 0.0862 - val_loss: 0.2831 - val_mean_squared_error: 0.2831\n",
      "Epoch 32/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0815 - mean_squared_error: 0.0815 - val_loss: 0.3021 - val_mean_squared_error: 0.3021\n",
      "Epoch 33/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0862 - mean_squared_error: 0.0862 - val_loss: 0.2636 - val_mean_squared_error: 0.2636\n",
      "Epoch 34/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0884 - mean_squared_error: 0.0884 - val_loss: 0.2790 - val_mean_squared_error: 0.2790\n",
      "Epoch 35/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0863 - mean_squared_error: 0.0863 - val_loss: 0.2714 - val_mean_squared_error: 0.2714\n",
      "Epoch 36/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0825 - mean_squared_error: 0.0825 - val_loss: 0.2605 - val_mean_squared_error: 0.2605\n",
      "Epoch 37/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0667 - mean_squared_error: 0.0667 - val_loss: 0.2754 - val_mean_squared_error: 0.2754\n",
      "Epoch 38/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0658 - mean_squared_error: 0.0658 - val_loss: 0.2646 - val_mean_squared_error: 0.2646\n",
      "Epoch 39/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0648 - mean_squared_error: 0.0648 - val_loss: 0.2881 - val_mean_squared_error: 0.2881\n",
      "Epoch 40/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0563 - mean_squared_error: 0.0563 - val_loss: 0.2732 - val_mean_squared_error: 0.2732\n",
      "Epoch 41/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0560 - mean_squared_error: 0.0560 - val_loss: 0.2598 - val_mean_squared_error: 0.2598\n",
      "Epoch 42/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0596 - mean_squared_error: 0.0596 - val_loss: 0.3039 - val_mean_squared_error: 0.3039\n",
      "Epoch 43/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0560 - mean_squared_error: 0.0560 - val_loss: 0.2813 - val_mean_squared_error: 0.2813\n",
      "Epoch 44/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0437 - mean_squared_error: 0.0437 - val_loss: 0.2698 - val_mean_squared_error: 0.2698\n",
      "Epoch 45/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.2645 - val_mean_squared_error: 0.2645\n",
      "Epoch 46/2000\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.3044 - val_mean_squared_error: 0.3044\n",
      "Epoch 47/2000\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.2586 - val_mean_squared_error: 0.2586\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 2000\n",
    "logs = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size, epochs=epochs,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_mean_squared_error', mode='min', patience=30,  restore_best_weights=True)\n",
    "                            ],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def build_sequences(df, window=200, stride=200):\n",
    "    # Sanity check to avoid runtime errors\n",
    "    assert window % stride == 0\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    for id in df['id'].unique():\n",
    "        # Take only meaningful features\n",
    "        temp = df[df['id'] == id][['x_axis','y_axis','z_axis']].values\n",
    "        # Save the label\n",
    "        label = df[df['id'] == id]['activity'].values[0]\n",
    "        print(str(id), \"  \", str(label))\n",
    "        # Compute padding length\n",
    "        padding_len = window - len(temp)%window\n",
    "        # Create padding and concatenate it\n",
    "        padding = np.zeros((padding_len,3), dtype='float64')\n",
    "        temp = np.concatenate((temp,padding))\n",
    "        # Build features windows with their corresponging labels\n",
    "        idx = 0\n",
    "        while idx+window <= len(temp):\n",
    "            dataset.append(temp[idx:idx+window])\n",
    "            labels.append(label)\n",
    "            idx += stride\n",
    "    dataset = np.array(dataset)\n",
    "    labels = np.array(labels)\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(df, target_labels=['BC'], window=200, stride=200, telescope=100):\n",
    "    # Sanity check to avoid runtime errors\n",
    "    assert window % stride == 0\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    temp_df = df.copy().values\n",
    "    temp_label = df[target_labels].copy().values\n",
    "    padding_len = len(df)%window\n",
    "\n",
    "    if(padding_len != 0):\n",
    "        # Compute padding length\n",
    "        padding_len = window - len(df)%window\n",
    "        padding = np.zeros((padding_len,temp_df.shape[1]), dtype='float32')\n",
    "        temp_df = np.concatenate((padding,df))\n",
    "        padding = np.zeros((padding_len,temp_label.shape[1]), dtype='float32')\n",
    "        temp_label = np.concatenate((padding,temp_label))\n",
    "        assert len(temp_df) % window == 0\n",
    "\n",
    "    for idx in np.arange(0,len(temp_df)-window-telescope,stride):\n",
    "        dataset.append(temp_df[idx:idx+window])\n",
    "        labels.append(temp_label[idx+window:idx+window+telescope])\n",
    "\n",
    "    dataset = np.array(dataset)\n",
    "    labels = np.array(labels)\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = build_sequences(x_train, target_labels, window, stride, telescope)\n",
    "X_test, y_test = build_sequences(x_test, target_labels, window, stride, telescope)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 8\n",
    "SEQUENCE_DIM = x_train.shape[-1]\n",
    "RNN_CELL_DIM = 8\n",
    "HIDDEN_DIM = 8\n",
    "sequences = tf.keras.layers.Input(shape=[SEQUENCE_LENGTH, SEQUENCE_DIM])\n",
    "\n",
    "layer = keras.layers.LSTM(RNN_CELL_DIM, return_sequences=True)(sequences)\n",
    "\n",
    "layer = keras.layers.Dense(HIDDEN_DIM, activation='relu')(layer)\n",
    "\n",
    "predictions = keras.layers.Dense(1, activation=None)(layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=sequences, outputs=predictions)\n",
    "model.compile(\n",
    "            optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss=tf.losses.mean_squared_error,\n",
    "            metrics=[tf.metrics.mean_squared_error],\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
