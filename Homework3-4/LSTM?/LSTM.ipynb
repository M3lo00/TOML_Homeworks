{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Long Short â€“ Term Memory (LSTM) is a RNN architecture that developed to overcome the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(\"filled.csv\")\n",
    "data = data.set_index(pd.to_datetime(data['date']))\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_index()\n",
    "data = data.reset_index(drop=True)\n",
    "dates = data[\"date\"]\n",
    "data = data.drop([\"date\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_dataframe(df, columns):\n",
    "    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17,17))\n",
    "    for i, col in enumerate(columns):\n",
    "        axs[i].plot(df[col])\n",
    "        axs[i].set_title(col)\n",
    "    plt.show()\n",
    "inspect_dataframe(data, data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 6 # theshold a little high to retain some outliers\n",
    "z_scores = np.abs((data - data.mean()) / data.std())\n",
    "outliers = (z_scores > threshold).any(axis=1)\n",
    "for column in data.columns:\n",
    "    column_median = data[column].median()\n",
    "    data.loc[outliers, column] = column_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data, this is a rudimentary MinMaxScaler\n",
    "max_df = data.max()\n",
    "min_df = data.min()\n",
    "\n",
    "data_norm = (data - min_df)/(max_df - min_df)\n",
    "data_norm = pd.DataFrame(data_norm, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(df, target_labels=['BC'], window=200, stride=200):\n",
    "    # Sanity check to avoid runtime errors\n",
    "    assert window % stride == 0\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    temp_df = df.drop(['BC'], axis=1).copy().values\n",
    "    temp_label = df[target_labels].copy().values\n",
    "    # padding_len = len(df) % window\n",
    "    \n",
    "    # if padding_len != 0:\n",
    "    #     # Compute padding length\n",
    "    #     padding_len = window - len(df) % window\n",
    "    #     padding = np.zeros((padding_len, temp_df.shape[1]), dtype='float32')\n",
    "    #     temp_df = np.concatenate((padding, temp_df))\n",
    "    #     padding = np.zeros((padding_len,1), dtype='float32')\n",
    "    #     #padding = np.zeros((padding_len, temp_label.shape[1]), dtype='float32')\n",
    "    #     temp_label = np.concatenate((padding, temp_label))\n",
    "    #     assert len(temp_df) % window == 0\n",
    "\n",
    "    # Build sequences and labels\n",
    "    for i in range(0, len(temp_df) - window + 1, stride):\n",
    "        dataset.append(temp_df[i:i + window])\n",
    "        labels.append(temp_label[i:i + window])\n",
    "\n",
    "    return np.array(dataset), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 24*10\n",
    "val_size = 24*15\n",
    "window=24*8\n",
    "stride=2\n",
    "\n",
    "train = data_norm.iloc[val_size:-test_size]\n",
    "val= data_norm.iloc[0:val_size]\n",
    "test = data_norm.iloc[-test_size:]\n",
    "\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "# data_norm.describe()\n",
    "X_train, y_train = build_sequences(train, window=window, stride=stride)\n",
    "X_val, y_val = build_sequences(val, window=window, stride=stride)\n",
    "X_test, y_test = build_sequences(test, window=window, stride=stride)\n",
    "X_train.shape, y_train.shape,  X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "#tf.config.set_visible_devices([], 'GPU') #disables GPU\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50,return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[-1])))\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_dataframe(data_norm, data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train, \n",
    "    batch_size = batch_size,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True),\n",
    "        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-5)\n",
    "                            ],\n",
    "    epochs = epochs\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 192, 1), (25, 192, 1))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.047264987665346335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "y_pred\n",
    "mae = mean_absolute_error(y_test.reshape(-1, 1), y_pred.reshape(-1, 1))\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(X_test)\n",
    "# y_val_actual = scaler.inverse_transform(np.hstack((y_val.reshape(-1, 1), X_test[:, -1])))[:, 0]\n",
    "# y_pred_actual = scaler.inverse_transform(np.hstack((y_pred.reshape(-1, 1), X_test[:, -1])))[:, 0]\n",
    "\n",
    "# print(y_val_actual)\n",
    "# print(y_pred_actual)\n",
    "\n",
    "# mae = mean_absolute_error(y_val_actual, y_pred_actual)\n",
    "# mse = mean_squared_error(y_val_actual, y_pred_actual)\n",
    "# rmse = np.sqrt(mse)\n",
    "\n",
    "# print('MAE:', mae)\n",
    "# print('MSE:', mse)\n",
    "# print('RMSE:', rmse)\n",
    "\n",
    "# # Visualize the model's predictions\n",
    "# plt.plot(y_val_actual, label='Actual')\n",
    "# plt.plot(y_pred_actual, label='Predicted')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
